# Hints — Flash Attention

## The simplest possible structure (one thread per query row)

```cuda
__global__ void flash_attention_fwd(const float *Q, const float *K,
                                     const float *V, float *O,
                                     int N, int d) {
    int tid = threadIdx.x;
    int q_row = blockIdx.x * Br + tid;
    if (q_row >= N) return;

    float scale = 1.0f / sqrtf((float)d);

    // Per-query state
    float m_i = -FLT_MAX;
    float l_i = 0.0f;
    // Output accumulator (in registers or local memory)
    float O_i[MAX_D];  // MAX_D = max head dim, e.g. 128
    for (int k = 0; k < d; k++) O_i[k] = 0.0f;

    // Load Q row once
    float q[MAX_D];
    for (int k = 0; k < d; k++) q[k] = Q[q_row * d + k];

    // Loop over K/V tiles
    for (int j = 0; j < N; j += Bc) {
        // Compute Sij = q · K[j:j+Bc]^T for this tile
        float S[Bc_MAX];
        float m_ij = -FLT_MAX;
        for (int c = 0; c < Bc && j + c < N; c++) {
            float dot = 0;
            for (int k = 0; k < d; k++)
                dot += q[k] * K[(j + c) * d + k];
            S[c] = dot * scale;
            m_ij = fmaxf(m_ij, S[c]);
        }

        // Compute P_ij = exp(S - m_ij) and l_ij = sum(P_ij)
        float l_ij = 0;
        float P[Bc_MAX];
        for (int c = 0; c < Bc && j + c < N; c++) {
            P[c] = expf(S[c] - m_ij);
            l_ij += P[c];
        }

        // Update running statistics
        float m_new = fmaxf(m_i, m_ij);
        float l_new = l_i * expf(m_i - m_new) + l_ij * expf(m_ij - m_new);

        // Rescale old output and add new contribution
        float scale_old = l_i * expf(m_i - m_new) / l_new;
        float scale_new = expf(m_ij - m_new) / l_new;
        for (int k = 0; k < d; k++) {
            float pv = 0;
            for (int c = 0; c < Bc && j + c < N; c++)
                pv += P[c] * V[(j + c) * d + k];
            O_i[k] = O_i[k] * scale_old + pv * scale_new;
        }

        m_i = m_new;
        l_i = l_new;
    }

    // Write output
    for (int k = 0; k < d; k++)
        O[q_row * d + k] = O_i[k];
}
```

This is NOT efficient (no shared memory, no parallelism over d), but it's CORRECT.
Get this working first, then optimize with shared memory and collaborative loading.

## Causal mask addition
Just add after computing S:
```cuda
if (j + c > q_row) S[c] = -FLT_MAX;  // or -INFINITY
```
