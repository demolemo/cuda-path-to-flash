# cuda-path-to-flash ðŸ”¥

Self-taught CUDA course â€” from "what's a thread?" to writing Flash Attention from scratch.

No university degree needed. Just a GPU and stubbornness.

## Roadmap

| # | Topic | Goal | Status |
|---|-------|------|--------|
| 01 | GPU Architecture & Basics | Understand threads/blocks/warps, write first kernel | ðŸ”¨ |
| 02 | Memory Hierarchy | Shared memory, coalescing, bank conflicts | â¬œ |
| 03 | Parallel Patterns | Reduction, scan, histogram | â¬œ |
| 04 | Matmul: Naive â†’ Tiled | Write matmul from 1% to ~30% cuBLAS | â¬œ |
| 05 | Matmul: Vectorized & Beyond | Vectorized loads, double buffering, warp tiling â†’ 60%+ cuBLAS | â¬œ |
| 06 | Online Softmax | Numerically stable softmax, online algorithm | â¬œ |
| 07 | Flash Attention | Tiling + online softmax + fused kernel | â¬œ |

## Key Resources

- **PMPP** â€” *Programming Massively Parallel Processors* (Kirk & Hwu) â€” the textbook
- **Simon Boehm** â€” [How to Optimize a CUDA Matmul Kernel](https://siboehm.com/articles/22/CUDA-MMM) â€” the matmul bible
- **Tri Dao** â€” Flash Attention [1](https://arxiv.org/abs/2205.14135) & [2](https://arxiv.org/abs/2307.08691) papers
- **Milakov & Gimelshein** â€” [Online normalizer calculation for softmax](https://arxiv.org/abs/1805.02867)
- **Towernest** â€” CUDA kernel optimization deep dives
- **Lei Mao** â€” [leimao.github.io](https://leimao.github.io/) â€” incredible CUDA blog
- **Mark Harris / NVIDIA** â€” Classic CUDA blog posts (reduction, shared memory, transpose)
- **GPU Puzzles** â€” [srush/GPU-Puzzles](https://github.com/srush/GPU-Puzzles) â€” interactive exercises

ðŸ‘‰ **[RESOURCES.md](RESOURCES.md)** â€” The full resource bible: books, blogs, papers, repos, videos, courses, people to follow. Everything.

## How to Use

```bash
cd 01-gpu-basics
make test        # run correctness checks
make bench       # run benchmarks
```

Each module has exercises (skeleton â†’ fill in the kernel), tests, benchmarks, and hints you should only open when stuck.
